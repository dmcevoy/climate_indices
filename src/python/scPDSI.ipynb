{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Abstract</h3>\n",
    "\n",
    "In order to compute a self-calibrated Palmer Drought Severity Index we'll use the methodology described in Palmer 1965, Wells 2004, and Alley 1983. Starting from precipitation, potential evapotranspiration, and soil water capacity we will compute both Z-index and PDSI values, self calibrated to the geographical location over a specified calibration period.\n",
    "\n",
    "<h4>Input/Output</h4>\n",
    "We'll read and write data sets from NetCDF so we'll need functions to read and write to NetCDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named netCDF4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9380e7ebae37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named netCDF4"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import netCDF4\n",
    "from numpy import ndarray\n",
    "import numba\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# set up a global logger\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "def write_dataset(output_file,\n",
    "                  template_dataset,\n",
    "                  pdsi_data):\n",
    "\n",
    "    # get the coordinates from the template file\n",
    "    times = template_dataset.variables['time'][:]\n",
    "    lons = template_dataset.variables['lon'][:]\n",
    "    lats = template_dataset.variables['lat'][:]\n",
    "\n",
    "    #TODO verify the shape of the PDSI data values array against the template dimensions\n",
    "\n",
    "    # open the output file for writing, set its dimensions and variables\n",
    "    dataset = netCDF4.Dataset(output_file, 'w')\n",
    "    dataset.createDimension('time', None)\n",
    "    dataset.createDimension('lon', len(lons))\n",
    "    dataset.createDimension('lat', len(lats))\n",
    "\n",
    "    # set some global group attributes\n",
    "    dataset.title = 'Self-calibrated Palmer Drought Severity Index (scPDSI)'\n",
    "    dataset.source = 'calculation using a new NCEI Python version of code originally developed in Matlab by J. Wolf, University of Idaho'\n",
    "    dataset.institution = 'National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce'\n",
    "    dataset.standard_name_vocabulary = 'CF Standard Name Table (v26, 08 November 2013)'\n",
    "    dataset.date_created = str(datetime.now())\n",
    "    dataset.date_modified = str(datetime.now())\n",
    "    dataset.Conventions = 'CF-1.6'\n",
    "\n",
    "    # create a time coordinate variable with an increment per month of the period of record\n",
    "    time_variable = dataset.createVariable('time', 'i4', ('time',))\n",
    "    time_variable.long_name = 'time'\n",
    "    time_variable.standard_name = 'time'\n",
    "    time_variable.calendar = template_dataset.variables['time'].calendar\n",
    "    time_variable.units = template_dataset.variables['time'].units\n",
    "    time_variable[:] = times\n",
    "\n",
    "    # create the lon coordinate variable\n",
    "    lon_variable = dataset.createVariable('lon', 'f4', ('lon',))\n",
    "    lon_variable.long_name = 'longitude'\n",
    "    lon_variable.standard_name = 'longitude'\n",
    "    lon_variable[:] = lons\n",
    "    try:\n",
    "        units = template_dataset.variables['lon'].units\n",
    "        if units is not None:\n",
    "            lon_variable.units = units\n",
    "    except AttributeError as e:\n",
    "        logger.info('No units for longitude coordinate variable in the template data set NetCDF')\n",
    "\n",
    "    # create the lat coordinate variable\n",
    "    lat_variable = dataset.createVariable('lat', 'f4', ('lat',))\n",
    "    lat_variable.long_name = 'latitude'\n",
    "    lat_variable.standard_name = 'latitude'\n",
    "    lat_variable[:] = lats\n",
    "    try:\n",
    "        units = template_dataset.variables['lat'].units\n",
    "        if units is not None:\n",
    "            lat_variable.units = units\n",
    "    except AttributeError as e:\n",
    "        logger.info('No units for latitude coordinate variable in the template data set NetCDF')\n",
    "\n",
    "    # create the variable\n",
    "    variable = dataset.createVariable('pdsi',\n",
    "                                      'f4',\n",
    "                                      ('time', 'lon', 'lat'),\n",
    "                                      fill_value=np.NaN,\n",
    "                                      zlib=True,\n",
    "                                      least_significant_digit=3)\n",
    "\n",
    "    # load the data into the variable\n",
    "    variable[:] = pdsi_data\n",
    "\n",
    "    # close the output NetCDF file\n",
    "    dataset.close()\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "def load_data(input_dataset,\n",
    "              variable_name,\n",
    "              time_index):\n",
    "\n",
    "    # make sure the data set has the dimensions and variables we expect\n",
    "    if (not input_dataset.dimensions.has_key('lat')):\n",
    "        logger.critical(\"Input data set file is missing the 'lat' dimension\")\n",
    "        return 1\n",
    "    if (not input_dataset.dimensions.has_key('lon')):\n",
    "        logger.critical(\"Input data set file is missing the 'lon' dimension\")\n",
    "        return 1\n",
    "    if (not input_dataset.dimensions.has_key('time')):\n",
    "        logger.critical(\"Input data set file is missing the 'time' dimension\")\n",
    "        return 1\n",
    "    if (not input_dataset.variables.has_key('lat')):\n",
    "        logger.critical(\"Input data set file is missing the 'lat' variable\")\n",
    "        return 1\n",
    "    if (not input_dataset.variables.has_key('lon')):\n",
    "        logger.critical(\"Input data set file is missing the 'lon' variable\")\n",
    "        return 1\n",
    "    if (not input_dataset.variables.has_key('time')):\n",
    "        logger.critical(\"Input data set file is missing the 'time' variable\")\n",
    "        return 1\n",
    "    if (not input_dataset.variables.has_key(variable_name)):\n",
    "        logger.critical('Input data set file is missing the ' + variable_name + ' variable')\n",
    "        return 1\n",
    "\n",
    "    #TODO verify that the dimensions are in [time, lon, lat] order\n",
    "\n",
    "    try:\n",
    "        # pull the data from the variable\n",
    "        data = input_dataset.variables[variable_name][time_index:time_index + 1:1, :, :]\n",
    "    except Exception as ex:\n",
    "        message = 'Unable to read data for the variable named \\'{var}\\':  Cause: {cause}'.format(var=variable_name, cause=ex.__cause__)\n",
    "        logger.critical(message)\n",
    "        raise RuntimeError(message)\n",
    "\n",
    "    return data\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "def extract_coords(datasets):\n",
    "\n",
    "    for dataset in datasets:\n",
    "\n",
    "        if (not dataset.dimensions.has_key('lat')):\n",
    "            message = \"Input data set file is missing the 'lat' dimension\"\n",
    "            logger.critical(message)\n",
    "            raise ValueError(message)\n",
    "            return 1\n",
    "        if (not dataset.dimensions.has_key('lon')):\n",
    "            message = \"Input data set file is missing the 'lon' dimension\"\n",
    "            logger.critical(message)\n",
    "            raise ValueError(message)\n",
    "        if (not dataset.dimensions.has_key('time')):\n",
    "            message = \"Input data set file is missing the 'time' dimension\"\n",
    "            logger.critical(message)\n",
    "            raise ValueError(message)\n",
    "        if (not dataset.variables.has_key('lat')):\n",
    "            message = \"Input data set file is missing the 'lat' variable\"\n",
    "            logger.critical(message)\n",
    "            raise ValueError(message)\n",
    "        if (not dataset.variables.has_key('lon')):\n",
    "            message = \"Input data set file is missing the 'lon' variable\"\n",
    "            logger.critical(message)\n",
    "            raise ValueError(message)\n",
    "        if (not dataset.variables.has_key('time')):\n",
    "            message = \"Input data set file is missing the 'time' variable\"\n",
    "            logger.critical(message)\n",
    "            raise ValueError(message)\n",
    "\n",
    "        #TODO make sure the data variable(s) have dimensions [time, lon, lat]\n",
    "\n",
    "        # on the first pass we get our \"reference\" set of lons, lats, and times, and all subsequent files should match\n",
    "        if 'lons' not in locals():\n",
    "            lons = dataset.variables['lon'][:]\n",
    "            lats = dataset.variables['lat'][:]\n",
    "            times = dataset.variables['time'][:]\n",
    "\n",
    "        else:\n",
    "            # get the lons and lats from the current file\n",
    "            comparison_lons = dataset.variables['lon'][:]\n",
    "            comparison_lats = dataset.variables['lat'][:]\n",
    "\n",
    "            # make sure that this file's lons and lats match up with the initial file\n",
    "            if not np.allclose(lons, comparison_lons, atol=0.00001):\n",
    "                message = 'Longitude values not matching between base file ' + files[0] + ' and ' + file\n",
    "                logger.critical(message)\n",
    "                raise ValueError(message)\n",
    "            if not np.allclose(lats, comparison_lats, atol=0.00001):\n",
    "                message = 'Latitude values not matching between base file ' + files[0] + ' and ' + file\n",
    "                logger.critical(message)\n",
    "                raise ValueError(message)\n",
    "\n",
    "    return times, lons, lats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
